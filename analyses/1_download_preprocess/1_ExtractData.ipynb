{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "breathing-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "from importlib import reload\n",
    "import itertools\n",
    "from itertools import count\n",
    "\n",
    "import sys\n",
    "\n",
    "# CHANGE PATHS\n",
    "sys.path.insert(1,'/scratch/c.c21013066/software/ukbb_parser/ukbb_parser')\n",
    "sys.path.insert(1,'/scratch/c.c21013066/software/ukbb_parser/ukbb_parser/shared_utils')\n",
    "import ukbb_parser as ukbb_parser\n",
    "import ukbb_phenotype_dataset as ukbb_phenotype_dataset\n",
    "from shared_utils.util import summarize\n",
    "# CHANGE PATHS\n",
    "sys.path.insert(1,'../../resources')\n",
    "import phenotypesnew as pheno_info\n",
    "# CHANGE PATHS\n",
    "sys.path.insert(1,'../../resources/utils')\n",
    "import _preprocess\n",
    "import _get_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "current-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/scratch/c.c21013066/data/ukbiobank'\n",
    "sample_path = f'{data_path}/sample/withGP/noOsteo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-convention",
   "metadata": {},
   "source": [
    "# Extract Death and Birth info for Prevalence/Incidence normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many subjects to extract (to test code before running on complete dataset (None))\n",
    "nrows = None\n",
    "\n",
    "# get demographics: this calls function from ukbb_parser and passes to it the fields you want to extract \n",
    "# (in this case all specified in phenotypes.py under DEMOGRAPHICS)\n",
    "# for now we use all data (ie do not exclude relatives, use all ethnicities)\n",
    "# for now do not load genotype meta data (needs imputed gene data at specific locations set in .ukbb_paths.py)\n",
    "# this returns eid (unique subject identifier), demographics (the fields we specified), covariates (some general stuff set by\n",
    "# author of ukbb_parser package)\n",
    "eid, demographics, covariates = ukbb_phenotype_dataset.create_phenotype_dataset(pheno_info.DEMOGRAPHICS,nrows=nrows,\n",
    "                                        parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},\n",
    "                                        no_kinship=False, only_caucasians=False)\n",
    "demographics['eid'] = eid\n",
    "demographics.set_index('eid',inplace=True)\n",
    "# ethnicity (like all other things) is coded with some numbers representing some meaning (eg 1 means white)\n",
    "# this function makes a dummy coded version of these codes and replaces it by their meaning, so we get\n",
    "# ethnicity_white, ethnicity_asian, ... as columns with 1 where true and 0 where false\n",
    "demographics = pd.merge(demographics,_preprocess.recode_ethnicity(demographics[['ethnicity']],1001),on='eid')\n",
    "# ukbb does not give date of birth so we combine year and month to one datetime and set for all 15 as day of birth\n",
    "demographics = _preprocess.get_birthdate(demographics)\n",
    "eid, death, covariates = ukbb_phenotype_dataset.create_phenotype_dataset(pheno_info.DEATH,nrows=nrows,\n",
    "                                       parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},\n",
    "                                        no_kinship=False, only_caucasians=False)\n",
    "death['eid'] = eid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-algeria",
   "metadata": {},
   "outputs": [],
   "source": [
    "death.to_csv(f'{data_path}/phenotypes/death.csv')\n",
    "demographics.to_csv(f'{data_path}/phenotypes/demo.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-croatia",
   "metadata": {},
   "source": [
    "# Extract Disease info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(_preprocess)\n",
    "reload(pheno_info)\n",
    "reload(ukbb_parser)\n",
    "reload(ukbb_phenotype_dataset)\n",
    "# how many subjects to extract (too test code before running on complete dataset (None))\n",
    "nrows = None\n",
    "\n",
    "eid, demographics, covariates = ukbb_phenotype_dataset.create_phenotype_dataset(pheno_info.DEMOGRAPHICS,nrows=nrows,\n",
    "                                        parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},\n",
    "                                        no_kinship=False, only_caucasians=False)\n",
    "demographics['eid'] = eid\n",
    "demographics.set_index('eid',inplace=True)\n",
    "demographics = pd.merge(demographics,_preprocess.recode_ethnicity(demographics[['ethnicity']],1001),on='eid')\n",
    "demographics = _preprocess.get_birthdate(demographics)\n",
    "\n",
    "# now repeat basically same thing for all other fields of interest\n",
    "eid, baseline, covariates = ukbb_phenotype_dataset.create_phenotype_dataset(pheno_info.ASSESSMENTS,nrows=nrows,\n",
    "                                        parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},\n",
    "                                        no_kinship=False, only_caucasians=False)\n",
    "baseline['eid'] = eid\n",
    "baseline.set_index('eid',inplace=True)\n",
    "baseline['visit'] = 0\n",
    "baseline['date_visit'] = pd.to_datetime(baseline['date_visit'],format='%Y-%m-%d',errors='coerce')\n",
    "\n",
    "# NEW ADD PRINCIPAL COMPONENTS FOR COVARIATES\n",
    "eid, pcs, covariates = ukbb_parser.create_dataset(pheno_info.GeneticPCs,nrows=nrows,parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},\n",
    "                                        no_kinship=False, only_caucasians=False)\n",
    "pcs['eid'] = eid\n",
    "pcs.set_index('eid',inplace=True)\n",
    "pcs.columns = [f'PC_{i}' for i in range(pcs.shape[1])]\n",
    "\n",
    "eid, icd10diagnoses, covariates = ukbb_phenotype_dataset.create_phenotype_dataset(pheno_info.DIAGNOSESICD10,nrows=nrows,\n",
    "                                        parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},\n",
    "                                        no_kinship=False, only_caucasians=False,code='19')\n",
    "icd10diagnoses['eid'] = eid\n",
    "icd10diagnoses.set_index('eid',inplace=True)\n",
    "eid, icd9diagnoses, covariates = ukbb_phenotype_dataset.create_phenotype_dataset(pheno_info.DIAGNOSESICD9,nrows=nrows,\n",
    "                                        parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},\n",
    "                                        no_kinship=False, only_caucasians=False,code='87')\n",
    "icd9diagnoses['eid'] = eid\n",
    "icd9diagnoses.set_index('eid',inplace=True)\n",
    "\n",
    "eid, selfdiagnoses, covariates = ukbb_phenotype_dataset.create_phenotype_dataset(pheno_info.DIAGNOSESSELF,nrows=nrows,\n",
    "                                        parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},\n",
    "                                        no_kinship=False, only_caucasians=False,code='6')\n",
    "selfdiagnoses['eid'] = eid\n",
    "selfdiagnoses.set_index('eid',inplace=True)\n",
    "\n",
    "pheno_info.run_gpdiagnosis(pheno_info.DIAGNOSESGP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get date of diagnosis info\n",
    "selfreport = _preprocess.get_selfreported_diagnoses(demographics,collapse_list=['parkinsonsdisease',\n",
    "                                                                              'dementiaalzheimerscognitiveimpairment',\n",
    "                                                                               'osteoarthritis','depression'],nrows=nrows,\n",
    "                                                  names=['ParkinsonDisease','AllCauseDementia','Osteoarthritis','Depression'])\n",
    "\n",
    "collapsegp = ['gp_ParkinsonDisease','gp_OtherParkinsonism','gp_AllCauseDementia','gp_AllCauseParkinsonism',\n",
    "              'gp_AlzheimerDisease','gp_MultipleSystemAtrophy','gp_ProgressiveSupranuclearPalsy',\n",
    "              'gp_FrontoTemporalDementia','gp_VascularDementia',\n",
    "              'gp_Dystonia','gp_Osteoarthritis','gp_Depression','gp_neurology','gp_nonHC']\n",
    "\n",
    "gp_diags = []\n",
    "for diagnosis in collapsegp:\n",
    "    diag = pd.read_csv(f'/scratch/c.c21013066/data/ukbiobank/record_level/{diagnosis}.csv',parse_dates=[f'{diagnosis}_date'])\n",
    "    gp_diags.append(diag)\n",
    "gp_diags = reduce(lambda left,right: pd.merge(left,right,on='eid',how='outer'), gp_diags)\n",
    "\n",
    "# extract codes for specific disorders (use info from above diagnostics)\n",
    "codes = pd.DataFrame.from_dict(pheno_info.DIAGNOSESICD10)\n",
    "codes_list = [codes.loc[codes['name']=='icd10_ParkinsonDisease','codings'].values.tolist()[0],\n",
    "              codes.loc[codes['name']=='icd10_OtherParkinsonism','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_AllCauseDementia','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_AllCauseParkinsonism','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_AlzheimerDisease','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_MultipleSystemAtrophy','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_ProgressiveSupranuclearPalsy','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_FrontoTemporalDementia','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_VascularDementia','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_Dystonia','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_Osteoarthritis','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_Depression','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_neurology','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_nonHC','codings'].values.tolist()[0]]\n",
    "collapseicd10 = ['icd10_ParkinsonDisease','icd10_OtherParkinsonism','icd10_AllCauseDementia','icd10_AllCauseParkinsonism',\n",
    "                 'icd10_AlzheimerDisease','icd10_MultipleSystemAtrophy','icd10_ProgressiveSupranuclearPalsy',\n",
    "                 'icd10_FrontoTemporalDementia','icd10_VascularDementia','icd10_Dystonia','icd10_Osteoarthritis','icd10_Depression','icd10_neurology','icd10_nonHC']\n",
    "date_diagicd10 = _preprocess.get_icd10diagnosis_source_date(codes_list,collapseicd10,nrows=nrows)\n",
    "date_diagicd10.columns = date_diagicd10.columns.map('_'.join).str.strip('_')\n",
    "\n",
    "codes = pd.DataFrame.from_dict(pheno_info.DIAGNOSESICD9)\n",
    "codes_list = [codes.loc[codes['name']=='icd9_ParkinsonDisease','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_AllCauseDementia','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_AllCauseParkinsonism','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_AlzheimerDisease','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_FrontoTemporalDementia','codings'].values.tolist()[0],\n",
    "            codes.loc[codes['name']=='icd9_VascularDementia','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_Dystonia','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_Osteoarthritis','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_Depression','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_neurology','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_nonHC','codings'].values.tolist()[0]]\n",
    "collapseicd9 = ['icd9_ParkinsonDisease','icd9_AllCauseDementia','icd9_AllCauseParkinsonism','icd9_AlzheimerDisease',\n",
    "           'icd9_FrontoTemporalDementia','icd9_VascularDementia','icd9_Dystonia','icd9_Osteoarthritis','icd9_Depression','icd9_neurology','icd9_nonHC']\n",
    "date_diagicd9 = _preprocess.get_icd9diagnosis_source_date(codes_list,collapseicd9,nrows=nrows)\n",
    "date_diagicd9.columns = date_diagicd9.columns.map('_'.join).str.strip('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get date for prodromal symptoms\n",
    "collapsegp = ['gp_Depression','gp_Anxiety','gp_Constipation','gp_ErectileDysfunction','gp_UrinaryIncontinence', 'gp_Hyposmia',\n",
    "             'gp_RBD','gp_OrthostaticHypotension']\n",
    "gp_diags = []\n",
    "for diagnosis in collapsegp:\n",
    "    diag = pd.read_csv(f'/scratch/c.c21013066/data/ukbiobank/record_level/{diagnosis}.csv',parse_dates=[f'{diagnosis}_date'])\n",
    "    gp_diags.append(diag)\n",
    "gp_diags = reduce(lambda left,right: pd.merge(left,right,on='eid',how='outer'), gp_diags)\n",
    "\n",
    "# extract codes for specific disorders (use info from above diagnostics)\n",
    "codes = pd.DataFrame.from_dict(pheno_info.DIAGNOSESICD10)\n",
    "codes_list = [codes.loc[codes['name']=='icd10_Depression','codings'].values.tolist()[0],\n",
    "              codes.loc[codes['name']=='icd10_Anxiety','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_Constipation','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_ErectileDysfunction','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_UrinaryIncontinence','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_Hyposmia','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_RBD','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd10_OrthostaticHypotension','codings'].values.tolist()[0]]\n",
    "collapseicd10 = ['icd10_Depression','icd10_Anxiety','icd10_Constipation','icd10_ErectileDysfunction',\n",
    "                 'icd10_UrinaryIncontinence','icd10_Hyposmia','icd10_RBD',\n",
    "                 'icd10_OrthostaticHypotension']\n",
    "date_diagicd10 = _preprocess.get_icd10diagnosis_source_date(codes_list,collapseicd10,nrows=nrows)\n",
    "date_diagicd10.columns = date_diagicd10.columns.map('_'.join).str.strip('_')\n",
    "\n",
    "codes = pd.DataFrame.from_dict(pheno_info.DIAGNOSESICD9)\n",
    "codes_list = [codes.loc[codes['name']=='icd9_Depression','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_Anxiety','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_Constipation','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_ErectileDysfunction','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_UrinaryIncontinence','codings'].values.tolist()[0],\n",
    "            codes.loc[codes['name']=='icd9_Hyposmia','codings'].values.tolist()[0],\n",
    "             codes.loc[codes['name']=='icd9_OrthostaticHypotension','codings'].values.tolist()[0]]\n",
    "collapseicd9 = ['icd9_Depression','icd9_Anxiety','icd9_Constipation','icd9_ErectileDysfunction',\n",
    "           'icd9_UrinaryIncontinence','icd9_Hyposmia','icd9_OrthostaticHypotension']\n",
    "date_diagicd9 = _preprocess.get_icd9diagnosis_source_date(codes_list,collapseicd9,nrows=nrows)\n",
    "date_diagicd9.columns = date_diagicd9.columns.map('_'.join).str.strip('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viral-knock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for HC definition we do not require age\n",
    "selfreport['selfreported_neurology_age'] = np.nan\n",
    "selfreport['selfreported_nonHC_age'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-benefit",
   "metadata": {},
   "source": [
    "## Combine data\n",
    "\n",
    "- after loading all you want, merge to one dataframe\n",
    "- use some functions of preprocess to define some new fields (visit age, age of diagnoses (only works when you use the date_diag way of grabbing info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine datasets\n",
    "# HERE ADD PCS TO MERGED DATA\n",
    "dfs = [demographics,baseline,icd10diagnoses,icd9diagnoses,selfdiagnoses,date_diagicd10,date_diagicd9,selfreport,pcs,gp_diags]\n",
    "merged = reduce(lambda left,right: pd.merge(left,right,on='eid',how='outer',suffixes=[\"_x\",'']), dfs)\n",
    "# transform date to age\n",
    "merged = _preprocess.get_visit_age(merged)\n",
    "merged = _preprocess.get_diagnosis_age(merged,diags=collapseicd10)\n",
    "merged = _preprocess.get_diagnosis_age(merged,diags=collapseicd9,source=['hospital'])\n",
    "merged = _preprocess.get_diagnosis_age(merged,diags=collapsegp,source=[])\n",
    "merged = merged.set_index(\"eid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify for which disorder you just extracted you want to create a csv file containing all subject with that disorder and\n",
    "# additional info on date/source\n",
    "reload(_preprocess)\n",
    "# whenever there is no selfreport code or ICD10 code for that, just put fillself/fillicd9 and it will be ignored\n",
    "icd10 = ['icd10_ParkinsonDisease','icd10_OtherParkinsonism','icd10_AllCauseDementia','icd10_AllCauseParkinsonism',\n",
    "                 'icd10_AlzheimerDisease','icd10_MultipleSystemAtrophy','icd10_ProgressiveSupranuclearPalsy',\n",
    "                 'icd10_FrontoTemporalDementia','icd10_VascularDementia','icd10_Dystonia','icd10_Osteoarthritis','icd10_Depression','icd10_neurology','icd10_nonHC']\n",
    "selfes = ['selfreported_ParkinsonDisease','fillself','selfreported_AllCauseDementia','selfreported_ParkinsonDisease',\n",
    "         'fillself','fillself','fillself',\n",
    "          'fillself','fillself','fillself','selfreported_Osteoarthritis','selfreported_Depression','selfreported_neurology','selfreported_nonHC']\n",
    "names = ['ParkinsonDisease','OtherParkinsonism','AllCauseDementia','AllCauseParkinsonism','AlzheimerDisease',\n",
    "        'MultipleSystemAtrophy','ProgressiveSupranuclearPalsy','FrontoTemporalDementia','VascularDementia','Dystonia','Osteoarthritis','Depression','neurology','nonHC']\n",
    "icd9 = ['icd9_ParkinsonDisease', 'fillicd9',\n",
    " 'icd9_AllCauseDementia',\n",
    " 'icd9_AllCauseParkinsonism',\n",
    " 'icd9_AlzheimerDisease', 'fillicd9','fillicd9',\n",
    " 'icd9_FrontoTemporalDementia',\n",
    " 'icd9_VascularDementia','icd9_Dystonia','icd9_Osteoarthritis','icd9_Depression','icd9_neurology','icd9_nonHC']\n",
    "gps = ['gp_ParkinsonDisease','gp_OtherParkinsonism','gp_AllCauseDementia','gp_AllCauseParkinsonism','gp_AlzheimerDisease','gp_MultipleSystemAtrophy','gp_ProgressiveSupranuclearPalsy',\n",
    "       'gp_FrontoTemporalDementia','gp_VascularDementia','gp_Dystonia','gp_Osteoarthritis','gp_Depression',\n",
    "      'gp_neurology','gp_nonHC']\n",
    "\n",
    "#DEFINE COVARIATES TO BE EXTRACTED\n",
    "covariates = np.hstack(['male',[f'PC_{i}' for i in range(20)],'date_birth'])\n",
    "\n",
    "# PASS COVARIATES SUCH THAT THEY ARE EXTRACTED\n",
    "_preprocess.extract_disorder_withGP(merged,icd10,icd9,selfes,gps,names,save=sample_path,covariates=covariates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-boost",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify for which disorder you just extracted you want to create a csv file containing all subject with that disorder and\n",
    "# additional info on date/source\n",
    "# whenever there is no selfreport code or ICD10 code for that, just put fillself/fillicd9 and it will be ignored\n",
    "icd10 = ['icd10_Anxiety','icd10_Constipation','icd10_ErectileDysfunction',\n",
    "                 'icd10_UrinaryIncontinence','icd10_Hyposmia','icd10_RBD',\n",
    "                 'icd10_OrthostaticHypotension']\n",
    "selfes = ['selfreported_Anxiety','selfreported_Constipation',\n",
    "         'selfreported_ErectileDysfunction','selfreported_UrinaryIncontinence','fillself',\n",
    "          'fillself','fillself']\n",
    "names = ['Anxiety','Constipation','ErectileDysfunction',\n",
    "                 'UrinaryIncontinence','Hyposmia','RBD',\n",
    "                 'OrthostaticHypotension']\n",
    "icd9 = ['icd9_Anxiety','icd9_Constipation','icd9_ErectileDysfunction',\n",
    "           'icd9_UrinaryIncontinence','icd9_Hyposmia','fillicd9','icd9_OrthostaticHypotension']\n",
    "gps = ['gp_Anxiety','gp_Constipation','gp_ErectileDysfunction','gp_UrinaryIncontinence', 'gp_Hyposmia',\n",
    "             'gp_RBD','gp_OrthostaticHypotension']\n",
    "\n",
    "#DEFINE COVARIATES TO BE EXTRACTED\n",
    "covariates = np.hstack(['male',[f'PC_{i}' for i in range(20)],'date_birth'])\n",
    "\n",
    "# CHANGE PATH\n",
    "# PASS COVARIATES SUCH THAT THEY ARE EXTRACTED\n",
    "_preprocess.extract_disorder_withGP(merged,icd10,icd9,selfes,gps,names,save=sample_path,covariates=covariates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-fantasy",
   "metadata": {},
   "source": [
    "# Manually ensure HC has no Osteoarthritis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "local-shadow",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/c.c21013066/docker/envs/py38R/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3155: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "hc = pd.read_csv(f'{sample_path}/healthy_not_nonHC.csv')\n",
    "osteo = pd.read_csv(f'{sample_path}/Osteoarthritis.csv')\n",
    "hc = hc.set_index('eid')\n",
    "hc = hc.drop(index=np.intersect1d(hc.index,osteo.eid))\n",
    "hc.to_csv(f'{sample_path}/healthy_not_nonHC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-warrant",
   "metadata": {},
   "source": [
    "## Manually ensure HC not treated for Parkinsonism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET VERBAL INTERVIEW MEDICATION INFO\n",
    "reload(pheno_info)\n",
    "reload(ukbb_parser)\n",
    "reload(_preprocess)\n",
    "# you need to add this to phenotypes.py to specify the data field ID for medications you want to extract\n",
    "#MEDICATION = [\n",
    "#    ('medication', 20003, 'raw')\n",
    "#]\n",
    "# You need to have the xlsx file I sent you saved in the CODINGS_DIR named ukbb_ATC_to_coding4.xls (need to convert xlsx to xls)\n",
    "# what it does is search for your specified ATC codes in that excel file and extract the corresponding ukbb codings\n",
    "# then from the loaded datafield MEDICATION (saved in raw) we check if a subject has ever reported a ukbb code correspoding to the ATC codes you are interested in\n",
    "# it returns a dataframe for all subjects with 1 column that is named here immunosuppressants with 1 if they ever took some, 0 else\n",
    "# You'll then have to merge it with your extracted PD group etc on 'eid' to know who of that group have taken the meds (you can do that in R)\n",
    "nrows = None\n",
    "eid, raw,covariates = ukbb_parser.create_dataset(pheno_info.MEDICATION, nrows = nrows,parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},\n",
    "                                        no_kinship=False, only_caucasians=False)\n",
    "raw[raw.isna()] = 0\n",
    "raw = raw.astype(int).astype(str)\n",
    "raw[raw=='0'] = 'nan'\n",
    "raw['eid'] = eid\n",
    "raw.set_index('eid',inplace=True)\n",
    "\n",
    "# all parkinsonism drugs \n",
    "drugs = _preprocess.extract_medication(raw,atc_code=['N04'],name='antiparkinsonism')\n",
    "drugs.to_csv(f'{data_path}/phenotypes/AntiParkinsonism.csv')\n",
    "\n",
    "# levodopa, \n",
    "drugs = _preprocess.extract_medication(raw,atc_code=['N04BA'],name='Dopa')\n",
    "drugs.to_csv(f'{data_path}/phenotypes/Dopa.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get GP INFO MEDICATION\n",
    "pheno_info.run_gpmedication(pheno_info.GPDRUGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "finite-tooth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(211289, 30)\n",
      "(211243, 29) (513, 3)\n",
      "(211149, 29) (1620, 2)\n"
     ]
    }
   ],
   "source": [
    "hc = pd.read_csv(f'{sample_path}/healthy_not_nonHC.csv')\n",
    "antiparkinsonism = pd.read_csv(f'{data_path}/record_level/gp_AntiParkinsonism_first.csv',parse_dates=['gp_AntiParkinsonism_date'])\n",
    "antiparkinsonism = antiparkinsonism[antiparkinsonism['gp_AntiParkinsonism']==1]\n",
    "antiparkinsonism2 = pd.read_csv(f'{data_path}/phenotypes/AntiParkinsonism.csv')\n",
    "antiparkinsonism2 = antiparkinsonism2[antiparkinsonism2['antiparkinsonism']==1]\n",
    "print(hc.shape)\n",
    "hc = hc.set_index('eid')\n",
    "hc = hc.drop(index=np.intersect1d(hc.index,antiparkinsonism.eid))\n",
    "print(hc.shape,antiparkinsonism.shape)\n",
    "hc = hc.drop(index=np.intersect1d(hc.index,antiparkinsonism2.eid))\n",
    "print(hc.shape,antiparkinsonism2.shape)\n",
    "hc.to_csv(f'{sample_path}/healthy_not_nonHC.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-night",
   "metadata": {},
   "source": [
    "# Extract Accelerometer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breathing-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = None\n",
    "\n",
    "eid, demographics, covariates = ukbb_phenotype_dataset.create_phenotype_dataset(pheno_info.DEMOGRAPHICS,nrows=nrows,parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},\n",
    "                                        no_kinship=False, only_caucasians=False)\n",
    "demographics['eid'] = eid\n",
    "demographics.set_index('eid',inplace=True)\n",
    "demographics = pd.merge(demographics,_preprocess.recode_ethnicity(demographics[['ethnicity']],1001),on='eid')\n",
    "demographics = _preprocess.get_birthdate(demographics)\n",
    "\n",
    "eid, baseline, covariates = ukbb_phenotype_dataset.create_phenotype_dataset(pheno_info.ASSESSMENTS,nrows=nrows,parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},\n",
    "                                        no_kinship=False, only_caucasians=False)\n",
    "baseline['eid'] = eid\n",
    "baseline.set_index('eid',inplace=True)\n",
    "baseline['visit'] = 0\n",
    "baseline['date_visit'] = pd.to_datetime(baseline['date_visit'],format='%Y-%m-%d',errors='coerce')\n",
    "\n",
    "eid, icd10diagnoses, covariates = ukbb_phenotype_dataset.create_phenotype_dataset(pheno_info.DIAGNOSESICD10,nrows=nrows,parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},no_kinship=False, only_caucasians=False,code='19')\n",
    "icd10diagnoses['eid'] = eid\n",
    "\n",
    "eid, accelerometer, covariates = ukbb_phenotype_dataset.create_phenotype_dataset(pheno_info.ACCELEROMETER,nrows,parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},\n",
    "                                        no_kinship=False, only_caucasians=False)\n",
    "accelerometer['eid'] = eid\n",
    "accelerometer['date_accelerometry'] = pd.to_datetime(accelerometer['date_accelerometry'],format='%Y-%m-%d',errors='coerce')\n",
    "accelerometer.set_index('eid',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine datasets\n",
    "dfs = [demographics,baseline,accelerometer,icd10diagnoses]\n",
    "merged = reduce(lambda left,right: pd.merge(left,right,on='eid'), dfs)\n",
    "merged = _preprocess.get_visit_age(merged)\n",
    "merged['accelerometry_age'] = (merged['date_accelerometry'] - merged['date_birth'])/ np.timedelta64(1,'Y')\n",
    "merged = merged.set_index('eid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-mambo",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(f'{data_oath}/phenotypes/demo_acc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-image",
   "metadata": {},
   "source": [
    "# Extract Lifestyle and Blood info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-battle",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023_02_07-19:21:41] Reading all dataset rows of 24 columns (for 10 fields)...\n",
      "[2023_02_07-19:24:54] Finished after 0:03:13.908705.\n",
      "[2023_02_07-19:24:54] Read a dataset of 502462 samples.\n",
      "[2023_02_07-19:24:55] Knowing of 63 samples who have wished to withdraw, 49 of them are in the loaded dataset. Filtering out these records, the dataset has reduced from 502462 to 502413 samples.\n",
      "[2023_02_07-19:24:55] Parsing field sex...\n",
      "[2023_02_07-19:24:55] Parsing field year_of_birth...\n",
      "[2023_02_07-19:24:55] To avoid the \"dummy variable trap\", removing the AC_leeds column (44186 matching records).\n",
      "[2023_02_07-19:24:56] Parsing field male...\n",
      "[2023_02_07-19:24:56] Parsing field year_birth...\n",
      "[2023_02_07-19:24:56] Parsing field month_birth...\n",
      "[2023_02_07-19:24:56] Parsing field country_birth...\n",
      "[2023_02_07-19:24:56] Parsing field handedness...\n",
      "[2023_02_07-19:24:56] Parsing field skin_color...\n",
      "[2023_02_07-19:24:56] Parsing field ethnicity...\n",
      "[2023_02_07-19:24:56] Parsing field TownsendDeprivationIndex...\n",
      "[2023_02_07-19:24:56] Parsing field EducationAge...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/c.c21013066/docker/envs/py38R/lib/python3.8/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "../../resources/utils/_preprocess.py:480: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[df[c]<0,c] = np.nan\n",
      "/scratch/c.c21013066/docker/envs/py38R/lib/python3.8/site-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  isetter(ilocs[0], value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023_02_07-19:24:58] Reading all dataset rows of 11 columns (for 4 fields)...\n",
      "[2023_02_07-19:27:19] Finished after 0:02:21.018065.\n",
      "[2023_02_07-19:27:19] Read a dataset of 502462 samples.\n",
      "[2023_02_07-19:27:19] Knowing of 63 samples who have wished to withdraw, 49 of them are in the loaded dataset. Filtering out these records, the dataset has reduced from 502462 to 502413 samples.\n",
      "[2023_02_07-19:27:19] Parsing field sex...\n",
      "[2023_02_07-19:27:19] Parsing field year_of_birth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../../resources/utils/_get_data.py:67: DtypeWarning: Columns (90,91,92) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  eid, baseline, covariates = ukbb_phenotype_dataset.create_phenotype_dataset(pheno_info.ASSESSMENTS,nrows=nrows,parse_dataset_covariates_kwargs={'use_genotyping_metadata':False},no_kinship=False, only_caucasians=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023_02_07-19:27:20] To avoid the \"dummy variable trap\", removing the AC_leeds column (44186 matching records).\n",
      "[2023_02_07-19:27:20] Parsing field date_visit...\n",
      "[2023_02_07-19:27:21] Parsing field site...\n",
      "[2023_02_07-19:27:22] Reading all dataset rows of 299 columns (for 8 fields)...\n",
      "[2023_02_07-19:31:46] Finished after 0:04:24.720634.\n",
      "[2023_02_07-19:31:46] Read a dataset of 502462 samples.\n",
      "[2023_02_07-19:31:48] Knowing of 63 samples who have wished to withdraw, 49 of them are in the loaded dataset. Filtering out these records, the dataset has reduced from 502462 to 502413 samples.\n",
      "[2023_02_07-19:31:48] Parsing field sex...\n",
      "[2023_02_07-19:31:48] Parsing field year_of_birth...\n",
      "[2023_02_07-19:31:48] To avoid the \"dummy variable trap\", removing the AC_leeds column (44186 matching records).\n",
      "[2023_02_07-19:32:00] Parsing the read dataset into an ICD-10 tree...\n",
      "[2023_02_07-19:32:00] Filtering the ICD-10 tree to keep only nodes descending from 944 specific codes...\n",
      "[2023_02_07-19:32:00] Remained with 1249 of 19154 nodes in the ICD-10 tree.\n",
      "[2023_02_07-19:33:37] Keeping only 1110 ICD-10 nodes that contain any samples (of 1249 nodes).\n",
      "[2023_02_07-19:33:37] Finished after 0:01:37.218167.\n",
      "[2023_02_07-19:33:39] icd10_Breast_cancer: Filtered 273328 females of 502413 total samples.\n",
      "[2023_02_07-19:33:39] icd10_EpithelialOvarian_cancer: Filtered 273328 females of 502413 total samples.\n",
      "[2023_02_07-19:33:39] icd10_Prostate_cancer: Filtered 229084 males of 502413 total samples.\n",
      "[2023_02_07-19:33:51] Reading all dataset rows of 75 columns (for 17 fields)...\n"
     ]
    }
   ],
   "source": [
    "nrows=None\n",
    "dfs = _get_data.get_risks(nrows=nrows)\n",
    "merged = _get_data.merge_data(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the subgroup you want to work with\n",
    "name = \"ParkinsonDisease\"\n",
    "exclude=['icd10_nonHC']\n",
    "drop_healthy = 'nonHC'\n",
    "levels = [0,1]\n",
    "covs = np.array(['visit_age','male','TownsendDeprivationIndex'])\n",
    "scale_covs = np.array([1,0,1]).astype(bool)\n",
    "keep = ['ParkinsonDisease_age','time_to_diagnosis']\n",
    "for modality in ['risk','blood']:\n",
    "    if modality == 'risk':\n",
    "        predictors_cat = ['AlcoholStatus_Current','AlcoholStatus_Previous','SmokeStatus_Current','SmokeStatus_Previous',\n",
    "                     'DaytimeSleepiness_Often','AlcoholFrequency_LessThanWeekly',\n",
    "                      'family_Stroke','family_Diabetes','family_Severedepression',\n",
    "                      'family_Alzheimersdiseasedementia','family_Parkinsonsdisease']\n",
    "        predictors_norm = np.hstack(['BMI','Waist_Circumference','Hip_Circumference','Diastolic_BloodPressure','PulseRate','BodyFat_Percentage',])\n",
    "        predictors = np.hstack([predictors_cat,predictors_norm])\n",
    "        scale_predictors = np.hstack([np.repeat([False],len(predictors_cat)),np.repeat([True],len(predictors_norm))])\n",
    "    elif modality == 'blood':\n",
    "        predictors = dfs[5].columns\n",
    "        scale_predictors = np.repeat([True],len(predictors))\n",
    "    print(predictors)\n",
    "    for name,exclude,drop_healthy in zip(['AllCauseDementia','AllCauseParkinsonism','AlzheimerDisease',\n",
    "        'MultipleSystemAtrophy','ProgressiveSupranuclearPalsy','FrontoTemporalDementia','VascularDementia','ParkinsonDisease',\n",
    "                                     'Dystonia','Osteoarthritis','Depression'],\n",
    "                                     [['icd10_nonHC'],['icd10_nonHC'],['icd10_nonHC'],['icd10_nonHC'],\n",
    "                                     ['icd10_nonHC'],['icd10_nonHC'],['icd10_nonHC'],['icd10_nonHC'],['icd10_nonHC'],\n",
    "                                     ['icd10_nonHC'],['icd10_nonHC']],\n",
    "                                    ['nonHC','nonHC','nonHC','nonHC','nonHC','nonHC','nonHC','nonHC','nonHC',\n",
    "                                    'nonHC','nonHC']):\n",
    "        keep = [f'{name}_age','time_to_diagnosis']\n",
    "        print(f'disease group: {name}, control group: no {drop_healthy}')\n",
    "        merged_ = _get_data.get_healthy_disorder(merged.copy(deep=True),name,covs=covs,\n",
    "                             predictors=predictors,incident=False,exclude=drop_healthy)\n",
    "        merged_clean = _preprocess.make_categorical(merged_,covs[~scale_covs],levels)\n",
    "        merged_clean = _preprocess.make_categorical(merged_clean,predictors[~scale_predictors],levels)\n",
    "        #drop columns with too many nan\n",
    "        predictors, scale_predictors = _preprocess.clean_predictors(merged_clean,predictors,scale_predictors,\n",
    "                             thresh=0.15)\n",
    "        # drop subjects with too many nan (here any nan)\n",
    "        merged_clean = _preprocess.clean_subjects(merged_clean,predictors,thresh=0)\n",
    "        merged_clean[np.hstack([predictors,name,covs,keep])].to_csv(f'{sample_path}/{name}_controlNo{drop_healthy}_{modality}.csv')\n",
    "        # subsample\n",
    "        #matched_sample = _get_data.get_matched(merged_clean,name,exclude=exclude,file=f'{name}_controlNo{drop_healthy}_match_{modality}.txt',save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
